---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

#Import Libraries
```{r message=FALSE, warning=FALSE}
library(aplore3)
library(caret)
library(dplyr)
library(tidyverse)
library(car)
require(ggthemes)
library(glmnet)
library(cowplot)
library(GGally)
library(ResourceSelection)
library(ROCR)
library(pROC)
library(doParallel)
library(qwraps2)
```


```{r}
cl <- makePSOCKcluster(7)
registerDoParallel(cl)

```

#Import Data
```{r}
data = glow_bonemed 
data = data.frame(data)
dim(data)
```

```{r}
data$sub_id = as.factor(data$sub_id)
data$site_id = as.factor(data$site_id)
data$phy_id = as.factor(data$phy_id)

```

#Split train and test set
```{r}
set.seed(66)
trainIndex <- createDataPartition(data$fracture, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train <- data[trainIndex,]
test  <- data[-trainIndex,]
dim(train)
dim(test)
```

# Objective do EDA and Simple Model
# EDA


All of the EDA will be done in the train data

View head of dataframe
```{r}
head(train)
```
Looking at Fracture balance
```{r}
# look for class imbalance
# The dataset is hevaily imbalance with more No's than Yes

data_classes = data %>% ggplot(aes(x=fracture)) + geom_bar() + theme_fivethirtyeight()
train_classes = train %>% ggplot(aes(x=fracture)) + geom_bar() + theme_fivethirtyeight()
test_classes = test %>% ggplot(aes(x=fracture)) + geom_bar() + theme_fivethirtyeight()

plot_grid(data_classes, train_classes, test_classes, labels = c("Overall Data", "Train Data", "Test Data"))

```


Let's look at pair plots from all the numeric variables
```{r}
train_numeric = train %>% select_if(is.numeric)
pairs(train[,2:8],col=as.factor(train$fracture))
```
Looking at a different view of pair plots for numerical variables. Excluding id's  
```{r message=FALSE, warning=FALSE}
ggpairs(train,columns=4:8,aes(colour=fracture))
```
Looking at box plot for different numerical variables per fracture or not
```{r}
boxplot_age = train %>% ggplot(aes(y=age, x=fracture)) + geom_boxplot() + ggtitle("age vs fracture") + theme_fivethirtyeight()

boxplot_weight = train %>% ggplot(aes(y=weight, x=fracture)) + geom_boxplot() + ggtitle("weight vs fracture")  + theme_fivethirtyeight()

boxplot_height = train %>% ggplot(aes(y=height, x=fracture)) + geom_boxplot() + ggtitle("height vs fracture") + theme_fivethirtyeight()

boxplot_bmi= train %>% ggplot(aes(y=bmi, x=fracture)) + geom_boxplot() + ggtitle("bmi vs fracture")  + theme_fivethirtyeight()

boxplot_fracscore= train %>% ggplot(aes(y=fracscore, x=fracture)) + geom_boxplot() + ggtitle("bmi vs fracture")  + theme_fivethirtyeight()

plot_grid(boxplot_age, boxplot_weight, boxplot_height, boxplot_bmi, boxplot_fracscore, nrow=2, ncol=2)
```
Lets look at bmi vs age per different categorical variables
```{r fig.height=10, fig.width=5}
# relation of bmi and age

age_bim_fracture = train %>% ggplot(aes(x=age, y=bmi, col=fracture)) + geom_point() + geom_smooth(method = 'loess' , formula = 'y ~ x'
) + ggtitle("bmi vs age") + xlab("age") + ylab("bmi") + theme_minimal() 

age_bim_premeno = train %>% ggplot(aes(x=age, y=bmi, col=premeno)) + geom_point() + geom_smooth(method = 'loess' , formula = 'y ~ x'
) + ggtitle("bmi vs age") + xlab("age") + ylab("bmi") + theme_minimal() 

age_bim_smoke = train %>% ggplot(aes(x=age, y=bmi, col=raterisk)) + geom_point() + geom_smooth(method = 'loess' , formula = 'y ~ x'
) + ggtitle("bmi vs age") + xlab("age") + ylab("bmi") + theme_minimal() 

age_bim_raterisk = train %>% ggplot(aes(x=age, y=bmi, col=smoke)) + geom_point() + geom_smooth(method = 'loess' , formula = 'y ~ x'
) + ggtitle("bmi vs age") + xlab("age") + ylab("bmi") + theme_minimal() 

plot_grid(age_bim_fracture, age_bim_premeno,age_bim_smoke, age_bim_raterisk, nrow=4, ncol=1)

```

Lets look at different numerica variables vs categorical variables per site id
The point os to investigate if site id had any impact
```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE}
bmi_frac_type = train %>% ggplot(aes(x=fracture, y=bmi, col=as.factor(site_id))) + geom_boxplot() + ggtitle("BMI for fracture type per site id")

age_frac_type = train %>% ggplot(aes(x=fracture, y=age, col=as.factor(site_id))) + geom_boxplot() + ggtitle("Age for fracture type per site id")

weight_frac_type = train %>% ggplot(aes(x=fracture, y=weight, col=as.factor(site_id))) + geom_boxplot() + ggtitle("Weight for fracture type per site id")

height_frac_type = train %>% ggplot(aes(x=fracture, y=height, col=as.factor(site_id))) + geom_boxplot() + ggtitle("Height for fracture type per site id")

plot_grid(bmi_frac_type, age_frac_type, weight_frac_type,height_frac_type, nrow=2, ncol=2)
```
```{r}
colnames(train)
```

```{r fig.height=10, fig.width=10}

options(qwraps2_markup = "markdown")
our_summary1 <-
  list("Age" =
       list("min"       = ~ min(age),
            "median"    = ~ median(age),
            "max"       = ~ max(age),
            "mean (sd)" = ~ qwraps2::mean_sd(age)),
       "Weight" =
       list("min"       = ~ min(weight),
            "median"    = ~ median(weight),
            "max"       = ~ max(weight),
            "mean (sd)" = ~ qwraps2::mean_sd(weight)),
       "Height" =
       list("min"       = ~ min(height),
            "median"    = ~ median(height),
            "max"       = ~ max(height),
            "mean (sd)" = ~ qwraps2::mean_sd(height)),
       "BMI" =
        list("min"       = ~ min(bmi),
             "median"    = ~ median(bmi),
                "max"       = ~ max(bmi),
                "mean (sd)" = ~ qwraps2::mean_sd(bmi)),
        "Frac Score" =
        list("min"       = ~ min(fracscore),
             "median"    = ~ median(fracscore),
                "max"       = ~ max(fracscore),
                "mean (sd)" = ~ qwraps2::mean_sd(fracscore))
       )

summary_table(train, our_summary1)
```
```{r}

our_summary1 <-
  list("Age" =
       list("min"       = ~ min(age),
            "median"    = ~ median(age),
            "max"       = ~ max(age),
            "mean (sd)" = ~ qwraps2::mean_sd(age)),
       "Weight" =
       list("min"       = ~ min(weight),
            "median"    = ~ median(weight),
            "max"       = ~ max(weight),
            "mean (sd)" = ~ qwraps2::mean_sd(weight)),
       "Height" =
       list("min"       = ~ min(height),
            "median"    = ~ median(height),
            "max"       = ~ max(height),
            "mean (sd)" = ~ qwraps2::mean_sd(height)),
       "BMI" =
        list("min"       = ~ min(bmi),
             "median"    = ~ median(bmi),
                "max"       = ~ max(bmi),
                "mean (sd)" = ~ qwraps2::mean_sd(bmi)),
        "Frac Score" =
        list("min"       = ~ min(fracscore),
             "median"    = ~ median(fracscore),
                "max"       = ~ max(fracscore),
                "mean (sd)" = ~ qwraps2::mean_sd(fracscore))
       )

summary_table(dplyr::group_by(train,fracture), our_summary1)

```
```{r}

options(qwraps2_markup = "markdown")
our_summary1 <-
  list("Age" =
       list("min"       = ~ min(age),
            "median"    = ~ median(age),
            "max"       = ~ max(age),
            "mean (sd)" = ~ qwraps2::mean_sd(age)),
       "Weight" =
       list("min"       = ~ min(weight),
            "median"    = ~ median(weight),
            "max"       = ~ max(weight),
            "mean (sd)" = ~ qwraps2::mean_sd(weight)),
       "Height" =
       list("min"       = ~ min(height),
            "median"    = ~ median(height),
            "max"       = ~ max(height),
            "mean (sd)" = ~ qwraps2::mean_sd(height)),
       "BMI" =
        list("min"       = ~ min(bmi),
             "median"    = ~ median(bmi),
                "max"       = ~ max(bmi),
                "mean (sd)" = ~ qwraps2::mean_sd(bmi))
       )

summary_table(train, our_summary1)
```

```


#Functions 
```{r}

fit_pred = function(model, test_x, test_y, m){
  if(m=="lasso"){
    fit.pred = predict(model, newx = test_x, type = "response")
    
  }
  if(m=="lda"){
     fit.pred= predict(model, test_x, type = "prob")
     fit.pred = fit.pred[,2]
  }
  
  if(m=="rf"){
     fit.pred= predict(model, test_x, type = "prob")
     fit.pred = fit.pred[,2]
  }
  
  if(m=="stepwise"){
    fit.pred = predict(model, newdata  = test_x, type = "response")

    
  }
  return(fit.pred)
  
}
make_predictions = function(model, test_x, test_y, m){
  
  fit.pred = fit_pred(model, test_x, test_y, m)
 
  
  
  results = prediction(fit.pred, test_y, 
                           label.ordering=c("No","Yes"))
  return(results)
}

classification_metrics = function(cutoff, model, model_type, test_x, test_y, m) {
  
  fit.pred = fit_pred(model, test_x, test_y, m)
 
  
  
  class<-factor(ifelse(fit.pred>cutoff,"Yes","No"),levels=c("No","Yes"))
  
  #Confusion Matrix for Lasso
  conf<-table(class,test_y)
  print(paste("Confusion matrix for ", model_type))
  print(conf)
  precision <- posPredValue(class, test_y, positive="Yes")
  recall <- sensitivity(class, test_y, positive="Yes")
  F1 <- (2 * precision * recall) / (precision + recall)
  print(paste("accuracy = ", round(mean(class==test_y) ,3), sep = ""))
  print(paste("precision = ", round(precision ,3), sep = ""))
  print(paste("recall = ", round(precision ,3), sep = ""))
  print(paste("F1 = ", round(F1 ,3), sep = ""))


}

roc_metrics = function(pred_results){
  
  roc = performance(pred_results, measure = "tpr", x.measure = "fpr")
  return(roc)
}

auc_metrics = function(pred_results) {
  auc <- performance(pred_results , measure = "auc")
  auc <- auc@y.values
  return(auc)
  
}
plot_roc = function (model_type, pred_results,x,y,c, ...) {
  roc = roc_metrics(pred_results)
  auc = auc_metrics(pred_results)
  plot(roc, colorize = c, ...)
  abline(a=0, b= 1)
  text(x = x, y = y, paste(model_type," AUC = ", round(auc[[1]],3), sep = ""))
}

```



# Build a new model

Lets train an interpretable logistic regression using the lasso technique
The point of this model is to be interpretable, meaning no exotic variables such as iteraction terms

```{r}
str(train)
```

```{r}

train.x <- model.matrix(fracture~  priorfrac + age + weight + height + bmi + premeno + momfrac + armassist + smoke+ raterisk + fracscore + bonemed + bonemed_fu, train)

train.y<-train[,15]


nFolds = 10 
set.seed(4)
foldid  = sample(rep(seq(nFolds), length.out = nrow(train.x)))
lambdas_to_try <- 10^seq(-5, 5, length.out = 2000)
set.seed(5)               
cvfit = cv.glmnet(train.x, train.y, 
                   family = "binomial", 
                  alpha=1,
                   type.measure = "auc", 
                   lambda = lambdas_to_try, 
                   nfolds = nFolds, 
                   foldid = foldid,
                  parallel = T
                  )

plot(cvfit)

coef(cvfit, s = "lambda.min")

print("CV AUC:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min
```



build a final interpretable model based on feature selection and lambda value selected above
```{r}
#For final model predictions go ahead and refit lasso using entire
#data set
#finalmodel = glmnet(train.x, train.y, family = "binomial",lambda=cvfit$lambda.min)
finalmodel_lasso<-glmnet(train.x, train.y, 
                   family = "binomial", 
                  alpha=1,
                   lambda = cvfit$lambda.min ) 
summary(finalmodel_lasso)
coef(finalmodel_lasso)
finalmodel_lasso
```
```{r}
finalmodel<-glm(fracture ~  priorfrac + height + bmi + premeno + momfrac  +
                  smoke + raterisk + raterisk + fracscore  +
                  bonemed_fu 
                  , data=train,family = binomial(link="logit"))
coef(finalmodel)
confint(finalmodel)
summary(finalmodel)
```

```{r}
train.x <- model.matrix(fracture~  priorfrac + age + weight + height + bmi + premeno + momfrac + armassist + smoke+ raterisk + fracscore + bonemed + bonemed_fu, train)

train.y<-train[,15]
preds_lasso = make_predictions(finalmodel_lasso, train.x, train.y, "lasso")
plot_roc("Lasso",preds_lasso,0.2,0.7,T)
classification_metrics(0.3, finalmodel_lasso, "Lasso", train.x, train.y, "lasso")
```

```{r}

vif(finalmodel)
```


```{r}
plot(finalmodel)
```


lets look at predictions for the lasso model
also looking at the roc plot to select the most optimal threhold for classification


```{r}
hoslem.test(finalmodel$y, fitted(finalmodel), g=10)

```

There is a large p-value so the test is a fit

```{r}
test.x <- model.matrix(fracture~  priorfrac + age + weight + height + bmi + premeno + momfrac + armassist + smoke+ raterisk + fracscore + bonemed + bonemed_fu, test)

test.y<-test[,15]
preds_lasso = make_predictions(finalmodel_lasso, test.x, test.y, "lasso")
plot_roc("Lasso",preds_lasso,0.2,0.7,T)
classification_metrics(0.3, finalmodel_lasso, "Lasso", test.x, test.y, "lasso")

```

lets look at model performance metrics
```{r}


```





# Stepwise regression

```{r}
library(leaps)
nvmax = 14
reg_sq=regsubsets(fracture~.-sub_id-site_id-phy_id-bonetreat,data=train, method="seqrep", nvmax=nvmax)
```

```{r}
par(mfrow=c(2,2))
cp<-summary(reg_sq)$cp
plot(1:(nvmax),cp,type="l",ylab="CP",xlab="# of predictors")
index<-which(cp==min(cp))
points(index,cp[index],col="red",pch=10)
bics<-summary(reg_sq)$bic
plot(1:(nvmax),bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==-0.05839447)
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg_sq)$adjr2
plot(1:(nvmax),adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
rss<-summary(reg_sq)$rss
plot(1:(nvmax),rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
```

```{r}

cbind(CP=summary(reg_sq)$cp,
      r2=summary(reg_sq)$rsq,
      Adj_r2=summary(reg_sq)$adjr2,
      BIC=summary(reg_sq)$bic,
      RSS = summary(reg_sq)$rss)
```


```{r}
coef(reg_sq, 8)
summary.out <- summary(reg_sq)
which.max(summary.out$adjr2)
summary.out$which[8,]
```




```{r}
#To deal with the redundamcy, I would throw the cylinder variable out and then see what happens
model.main<-glm(fracture ~raterisk+weight+bmi+premeno+momfrac+fracscore+bonemed_fu+smoke, data=train,family = binomial(link="logit"))
summary(model.main)
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
vif(model.main)
```

```{r}
#Residual diagnostics can be obtained using
plot(model.main)
```
```{r}
train_select = subset(train, select = c(weight,bmi,premeno,momfrac,fracscore,bonemed_fu,smoke,raterisk,fracture))
preds_step = make_predictions(model.main, train_select,train_select$fracture, "stepwise")
plot_roc("Step",preds_step,0.2,0.8,T)
classification_metrics(0.22, model.main, "Step", train_select,train_select$fracture, "stepwise")

```

```{r}
test_select = subset(test, select = c(weight,bmi,premeno,momfrac,fracscore,bonemed_fu,smoke,raterisk,fracture))
preds_step = make_predictions(model.main, test_select,test_select$fracture, "stepwise")
plot_roc("Step",preds_step,0.2,0.8,T)
classification_metrics(0.22, model.main, "Step", test_select,test_select$fracture, "stepwise")
```
lets look at model performance metrics

```{r}

```

```{r}

plot_roc("Step",preds_step,0.2,0.8,F, col="red")
par(new=T)
plot_roc("Lasso",preds_lasso,0.2,0.7,F, col="blue")
legend(0.7, 0.3,legend = c("Step","Lasso"), fill=c("red","blue"))
```

Objective 2

```{r}
# Feature Eng

#Square numerical variables 
train$ageSquared = train$age**2
train$weightSquared = train$weight**2
train$heigthSquared = train$height**2
train$bmiSquared = train$bmi**2

#Cubic numerical variables
train$ageCubic = train$age**3
train$weightCubic = train$weight**3
train$heigthCubic = train$height**3
train$bmiCubic= train$bmi**3


#Square numerical variables 
test$ageSquared = test$age**2
test$weightSquared = test$weight**2
test$heigthSquared = test$height**2
test$bmiSquared = test$bmi**2

#Cubic numerical variables
test$ageCubic = test$age**3
test$weightCubic = test$weight**3
test$heigthCubic = test$height**3
test$bmiCubic= test$bmi**3


# drop cols
train = subset(train, select = -c(sub_id,site_id,phy_id))
test = subset(test, select = -c(sub_id,site_id,phy_id))


head(train)
```

```{r}
# Scale numeric variables

preProcValues = preProcess(train, method = c("scale"))
train_transformed <- predict(preProcValues, train)
test_transformed <- predict(preProcValues, test)
test_transformed



```

```{r}

train.x <- model.matrix(fracture~ .*. , train_transformed)

train.y<-train_transformed[,12]


nFolds = 10 
set.seed(3)
foldid  = sample(rep(seq(nFolds), length.out = nrow(train.x)))
lambdas_to_try <- 10^seq(-3, 5, length.out = 2000)
set.seed(3)               
cvfit = cv.glmnet(train.x, train.y, 
                   family = "binomial", 
                   type.measure = "class", 
                   lambda = lambdas_to_try, 
                   nfolds = nFolds, 
                   foldid = foldid)

plot(cvfit)

coef(cvfit, s = "lambda.min")

print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

```

```{r message=FALSE, warning=FALSE}
finalmode_ob2 = glmnet(train.x, train.y, 
                   family = "binomial", 
                  alpha=1,
                   lambda = cvfit$lambda.min ) 
summary(finalmode_ob2)
coef(finalmode_ob2)
finalmodel_lasso
#coef(finalmode_ob2)
#confint(finalmode_ob2)
summary(finalmode_ob2)

```

```{r}
finalmode_ob2
```


```{r}
preds_ob2_lr = make_predictions(finalmode_ob2, train.x, train.y,"lasso")
plot_roc("Complex Lasso",preds_ob2_lr,0.2,0.8,T)
classification_metrics(0.20, finalmode_ob2, "Complex LR", train.x, train.y,"lasso")
```
```{r}
test.x <- model.matrix(fracture~ .*. , test_transformed)

test.y<-test_transformed[,12]
preds_ob2_lr = make_predictions(finalmode_ob2, test.x, test.y,"lasso")
plot_roc("Complex Lasso",preds_ob2_lr,0.2,0.8,T)
classification_metrics(0.20, finalmode_ob2, "Complex LR", test.x, test.y,"lasso")
```
```{r}
train
train_x = subset(train, select=-c(fracture))
train_x
```

# Build random forest model
```{r}
fitControl <- trainControl(
    method = 'repeatedcv',                  
    number = 10,
    repeats = 10,
    savePredictions = 'final',
    verboseIter = FALSE,
    classProbs = TRUE,
    search='grid'
) 

tunegrid <- expand.grid(.mtry = (1:15)) 

rpart_fit = train(fracture ~ ., data=train, method="rf", trControl = fitControl, tuneGrid = tunegrid)
print(rpart_fit)
```

```{r}


preds_rf = make_predictions(rpart_fit, train,train$fracture, "rf")
plot_roc("RF",preds_rf,0.2,0.8,T)
classification_metrics(0.5, rpart_fit, "Step", train,train$fracture, "rf")

```
```{r}


preds_rf = make_predictions(rpart_fit, test,test$fracture, "rf")
plot_roc("RF",preds_rf,0.2,0.8,T)
classification_metrics(0.22, rpart_fit, "RF", test,test$fracture, "rf")

```

```{r}
#library(MASS)
#library(pheatmap)
#fit.lda <- lda(fracture ~ ., data = train_transformed,  CV = TRUE)
#fit.lda


fitControl <- trainControl(
    method = 'repeatedcv',                  
    number = 10,
    repeats = 10,
    savePredictions = 'final',
    verboseIter = FALSE,
    classProbs = TRUE,
 
) 



lda_fit = train(fracture ~ ., data=train_transformed, method="lda", trControl = fitControl)
print(lda_fit)
```



```{r}
preds_lda = make_predictions(lda_fit, train_transformed,train_transformed$fracture, "lda")
plot_roc("LDA",preds_lda,0.2,0.8,T)
classification_metrics(0.22, lda_fit, "LDA", train_transformed,train_transformed$fracture, "lda")
```


```{r}
preds_rf = make_predictions(lda_fit, test_transformed,test_transformed$fracture, "lda")
plot_roc("LDA",preds_rf,0.2,0.8,T)
classification_metrics(0.22, lda_fit, "LDA", test_transformed,test_transformed$fracture, "lda")
```
